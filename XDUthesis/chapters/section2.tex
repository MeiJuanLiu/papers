\chapter{问题形式化描述与相关理论技术}

本章首先对问题做一个形式化描述，介绍网约车场景下的动态定价问题，描述动态定价的一般过程以及本文实验对比时使用到的基准算法。然后介绍强化学习的相关理论，包括马尔科夫决策过程以及深度多代理强化学习相关技术。
\section{问题形式化描述}
\subsection{问题描述}
（1）交通网络和乘车需求模型

本文将使用包括 $ N$ 个节点的全连通图来表示交通网络，图中的每个节点表示一个区域，可以看做订单的起点或者终点，并且每个节点都包含着一定数量的潜在乘客和司机，所有节点的集合使用 $G = \{v_i|i\in1,2,...,N\}$ 表示。本文假设任意两个节点之间的距离是相等的，事实上，本文所提出的方法很容易被扩展去考虑节点间距离不相等的情况，该假设是为了与已有基准算法的假设保持一致，方便实验的对比，本文拟最大化平台一天的收入，两个连续的时间步之间的间隔时长为 $\Delta_t$, 一天被划分成$T$个时间片。在每个时间步，潜在的乘客会出现在各个网络节点，并且会有一定数量的乘客会提交订单，所提交的订单会同一节点的可用的并愿意提供服务的司机所服务， 本文假设所有的订单会在一个时间步之后到达目的地。

当一个用户打开手机上的打车应用软件，选择其起点和目的地进行相应的乘车价格、通行时间、通行距离或者附近可用车辆的查询时，则表明这名乘客有打车的意向，不管该用户最终是否提交打车订单，平台会将其标记为一名潜在的乘客。在每个时间片内，平台可以统计得到各个节点的潜在乘车需求数，本文将从节点$i$到节点$j$的潜在乘车需求数使用$\Lambda_{ij}^t$表示。对于每个潜在乘客用户来说，用户会有一个对于本次行程的预留价格，预留价格影响着用户的行为。当平台给出的订单的价格低于其预留价格时，用户会愿意提交打车订单，反之，用户会放弃乘车退出打车平台。所以，在这个过程中可以看出，最终愿意提交订单的乘车需求与乘车价格有着一定的关系。动态定价问题大都需求设计一个模型来描述需求随价格变换之间的响应情况。需求模型的研究比较复杂且超过本文的范围，所以本文使用在常见的网约车动态定价策略研究中\cite{banerjee2015pricing,asghari2018adapt,chen2018pricing,guda2019your,bimpikis2019spatial}的需求模型来建模乘车需求随乘车价格变化的情况。

假设在乘客愿意付钱乘车的意愿上，所有乘客都是同质的。本文使用函数$f^r(p)$表示当乘车订单价格设置为$p$时，乘客愿意提交订单的概率, 其对应的累积分布函数为$F^r(p)$。本文将乘车价格的范围设置为$p\in[0,p_{max}]$，且价格按照起点和终点进行差异化定价。在时间步为$t$时，从起点$i$到终点$j$的乘车价格设置为$p_{ij}^t$，最终愿意提交订单的乘车需求数表示为$A_{ij}^t(p_{ij}^t)$, 形式化为：
\begin{equation}
A_{ij}^t(p_{ij}^t) = \Lambda_{ij}^t(1 - F^r(p_{ij}^t))
\end{equation}

（2）司机供应模型

司机与乘客一样，对订单价格也是敏感的。与乘客不同，乘客希望乘车价格越低越好，节省其乘车成本，而司机则希望乘车价格越高越好，这样便能提高其收入。司机对于每次的行程也有一个预留价格，当订单价格超过司机的预留价格，司机才会积极地加入平台，为平台广播给司机的订单提供服务。假设在$t$时，节点$i$的可用司机供应数为$V_i^t$。平台将指派一定比例的可用司机服务从节点$i$前往节点$j$的订单，假设指派比例为$b_{ij}^t$，则会有$b_{ij}^tV_i^t$数量的可用司机被平台调度来服务从节点$i$前往节点$j$的订单。$b_{ij}^t\in[0, 1]$， 并且满足$\sum_{i=1}^Nb_{ij}^t=1$的等式约束。 在$b_{ij}^tV_i^t$数量的可用司机中，最终只有预留价格不高于订单价格的司机才会愿意接单。本文使用函数$f^v(p)$表示当订单价格为$p$时，司机愿意接单的概率，其对应的累积分布函数为$F^v(p)$。所以，在$t$时，当交通网络中从地区$i$前往地区$j$的订单价格为$p_{ij}^t$时，最终的司机供应用$U_{ij}^t(p_{ij}^t)$表示，形式化为：
\begin{eqnarray}
U_{ij}^t(p_{ij}^t) = V_i^tb_{ij}^tF^v(p_{ij}^t)
\end{eqnarray}

（3）平台模型

平台通过调整订单价格来影响司机和乘客之间的行为，并充当司机和乘客之间的媒介，将已经提交的订单指派给一定数量的司机，并将乘客和愿意提供服务的司机进行匹配。在$t$时，假设从节点$i$前往节点$j$的乘车需求数为$A_{ij}^t(p_{ij}^t)$, 对应的愿意提供服务的司机供应数为$U_{ij}^t(p_{ij}^t)$, 当$U_{ij}^t(p_{ij}^t) < A_{ij}^t(p_{ij}^t)$, 供不应求下，平台按照一定的规则（如先来先服务）选择$U_{ij}^t(p_{ij}^t)$数量的乘客，为其匹配对应的司机，而剩下未被匹配的乘客会被平台放弃。同理， 当$U_{ij}^t(p_{ij}^t) > A_{ij}^t(p_{ij}^t)$, 供过于求时，所有提交的订单都会被匹配到对应的司机。 接到订单的司机通过服务乘客而转移到订单的目的地，完成其订单后，在目的地重新变得可用，未接单的司机则留在原处等待接单。 因此，不管是供不应求还是供过于求，最终平台在$t$时，设置从节点$i$前往节点$j$的订单价格为$p_{ij}^t$后，能服务的订单数为：
\begin{equation}
O_{ij}^t(p_{ij}^t) = \min\{A_{ij}^t(p_{ij}^t), U_{ij}^t(p_{ij}^t)\}
\end{equation}

平台会从已完成的订单所收到的钱中，提取$\lambda$比例的金钱作为其收入，剩下的$1-\lambda$比例的金钱由司机获得。在$t$时，平台在整个交通网络获得的收入表示为$Rev^t$，形式化为：
\begin{equation}
\label{Revt}
Rev^t = \lambda\sum_{i=1}^N\sum_{j=1}^Np_{ij}^tO_{ij}^t(p_{ij}^t) 
\end{equation}

（4）收入最大化问题

平台不能直接控制司机和乘客的行为，只能通过订单价格间接影响。因此网约车动态定价问题是通过设置合适的订单价格，来最大化平台的收入，提高订单响应率。之前大部分关于网约车动态定价的研究只把订单价格$p$作为优化的参数，而把车辆调度比例$b$作为常数项， 对每个时间步的价格进行单独的优化。
然而，由于司机在网络中的移动，会显著地影响未来可用司机在各地区的分布。仅优化价格而不考虑未来的供需状态会得到短视的定价策略，优化的是短期收入，限制了收入的提高。平台应该考虑优化长期收入，来将更多司机转移到未来需求激增的地区，服务即将到达的订单。所以平台需要同时优化每个订单的价格和车辆调度比例，制定前瞻性的定价策略和调度策略，从而平台的优化长期收入。
本文中拟打算最大化平台一天的总收入，所以本文需要解决下面的收入最大化问题：
\begin{equation}
\max_{p_{ij}^t,b_{ij}^t} \sum_{t=1}^T Rev^t = \max_{p_{ij}^t,b_{ij}^t} \sum_{t=1}^T\sum_{i=1}^N\sum_{j=1}^Np_{ij}^tO_{ij}^t(p_{ij}^t) 
\end{equation}
上式把$\lambda$去掉了，$\lambda$不是需要优化的参数，所以去掉并不影响最终的优化结果。

%\subsection{网约车动态定价的过程}
\subsection{已有基准算法}
已有的基准算法一般是基于数学优化的算法，通过求解当前时段的收入最大化问题，或者短期内的收入最大化问题。下面分别介绍属于这两个类型的经典算法。

（1） DPCRM算法

网约车平台（如优步、Lyft）一般根据网络的供需状况来实时地决定订单的价格。DPCRM（Dynamic Pricing for Current Revenue Maximization ，简称DPCRM）考虑网络当前的需求情况，当乘车需求高于可用司机供应时，订单的价格会提高以鼓励更多的司机加入平台。
DPCRM算法的订单价格根据起点进行差异化定价，且仅优化订单价格，车辆调度比例与从一个地区前往各个地区的订单比例保持一致。DPCRM通过优化每个时段的收入，得到每个时段各地区的最优价格${p_i^t}^*$， 形式化表示如下：
\begin{equation}
{p_i^t}^* = \max_{p_i^t} \sum_{i=1}^N p_i^tO_i^t(p_i^t)
\end{equation}
平台将每个时段的最优收入进行相加便可得到一天的总收入，即：
\begin{equation}
TotalRev = \sum_{t=1}^T\sum_{i=1}^N {p_i^t}^*O_i^t({p_i^t}^*)
\end{equation}

可以看出，DPCRM是短视的动态定价算法，没有同时优化司机的调度，有未将下一时段的供需情况纳入考虑。

（2）POD算法

POD（Predicting  demand at Origin \& Destination，简称POD）算法是文献\cite{asghari2018adapt}所提出的定价算法，其结合了下一时段的需求预测结果，通过降低当前时间步往未来会出现需求激增的地区的订单价格，来鼓励更多潜在乘客提交订单，从而使得未来能有更多可用司机出现在需求激增的地区。该算法依据订单的起点和终点的目的地进行差异化定价，且同时优化车辆的调度比例。

POD优化连续两个时段的总收入，当前时段的收入可以通过公式\ref{Revt}计算得到。
接单的司机通过接送乘客到目的地从而发生响应的转移，所以可以估计得到未来可用司机的在节点$i$的数量为：
\begin{equation}
V_i^{t+1} = V_i^t - \sum_{j=1}^NO_{ij}^t + \sum_{j=1}^N O_{ij}^t + \delta_i^{t+1}
\end{equation}
其中，$\sum_{j=1}^NO_{ij}^t$是从地区$i$载客离开的司机数量，$\sum_{j=1}^N O_{ij}^t$ 是从其它各个地区载客到达地区$i$的司机数，$\delta_i^{t+1}$是在$t+1$时段加入平台的司机数量和离开平台的司机数量的差。
结合下一时段的潜在需求预测结果$\Lambda^{t+1}$，便可以通过DPCRM算法求得下一时段的收入$Rev^{t+1}$
%\begin{equation}
%\alpha_i^{t+1} = \frac{ \sum_{i=1}^N \sum_{j=1}^N \Lambda_{ij}^{t+1} }{ \sum_{i=1}^N \sum_{j=1}^N \Lambda_{ij}^{t} }
%\end{equation}

POD算法通过优化以下目标函数求得$t$时的最优价格${p_{ij}^t}^*$和最优调度比例${b_{ij}^t}^*$：
\begin{equation}
{p_{ij}^t}^*, {b_{ij}^t}^* = \max_{{p_{ij}^t}, b_{ij}^t} (Rev^t + Rev^{t+1})
\end{equation}
POD最后的总收入通过把各时段的收入相加得到。

POD算法虽然在制定订单价格和车辆调度策略时具备一定的前瞻性，能有效缓解下一时间步由于某些地区需求激增所造成的的供需紧张的问题。但是，对于未来乘车需求较少的地区并且当前没有足够的车辆服务所有的订单时，POD会从价格和调度比例上抑制生成前往这些地区的订单。且由于司机只能通过载客发生转移，未载客的司机会继续原地等候订单，造成某些地区存在大量的可用车辆，而有些地区则供不应求。对于这种情况，仅考虑前后优化前后两个时段的短期收入是不够的，需要从更长远的视角来进行价格和调度比例的优化。POD优化前后两个时段的收入需要$O(N^3)$的时间复杂度，继续扩展优化更长期的收入则面临更高的计算复杂度。并且在继续建模$t+2,t+3,...$的收入也存在困难，缺乏扩展性和灵活性。

\section{相关理论技术}
%本论文基于强化学习将动态定价问题建模成马尔科夫决策过程，并且使用SAC算法找到最优的订单价格和车辆调度决策，因此本节将介绍马尔科夫决策的一般过程和常见

\subsection{马尔科夫决策过程}

强化学习问题可以通过建模成马尔科夫决策过程（Markov Decision Process，简称MDP）来形式化表示， MDP的基本思想是通过代理与环境进行交互来达到实际问题的目标。MDP可以用五元组$<\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma>$表示, 其中$\mathcal{S}$是状态空间，$\mathcal{A}$是动作空间。$\mathcal{R}$是奖励函数，将状态-动作对$(s_t,a_t)$映射到奖励值，$\mathcal{R}=\mathcal{R}(s_t,a_t):\mathcal{R}\times\mathcal{A}\to\mathbb{R}$。$\mathcal{P}=\mathcal{P}(s_{t+1|s_t,a_t}):\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$，是状态转移函数，描述了在给定当前状态$s_t$和动作$a_t$时，环境从$s_t$转移到$s_{t+1}$的概率，$\gamma\in[0,1]$是延迟因子，用于权衡即时奖励和未来收益。

代理与环境的交互发生在离散的连续时间步中，$t=0,1,2,3,...$，在每个时间步$t$，代理通过观测所处环境的状态$s_t\in \mathcal{S} $, 基于此状态并按照所遵循的策略$\pi(a_t|s_t):\mathcal{S}\times\mathcal{A}\to[0,1]$，选择一个动作$a_t\in \mathcal{A}$作用于环境。环境转移到下一个状态$s_{t+1}\sim\mathcal{P}(S_{t+1}|s_t,a_t)$，并给代理反馈回去一个奖励值$r_{t}\sim \mathcal{R}(s_t,u_t)\in \mathbb{R}$。代理与环境在每个时间步的交互过程如图\ref{FigMDP}所示。

\begin{figure*}[htbp]
	\centering
	
	
	\includegraphics[width=12cm]{figures/MDPs.png}
	
	\caption{代理与环境的交互过程}
	\label{FigMDP}
\end{figure*}

环境中的代理根据所收到的奖励信号更新其策略$\pi$，即更新给定状态$s$下，各个动作的概率分布，其目标是学习如何最大化每个情节的奖励值的累积折扣和的期望，累积折扣和称之为回报，形式化为：
\begin{equation}
G_t = \sum_{t=1}^{T}\gamma^{t-1}r_t
\end{equation}

代理通过最大化期望回报来找到最佳策略$\pi^*$，可形式化为，
\begin{equation}
\pi^* = \text{arg}\max_{\pi}\mathbb{E}_\pi[\sum_{t=1}^T\gamma^{t-1}r_t]
\end{equation}

目前存在很多强化学习算法来求解最佳策略，基于$Q$-表的方法如$Q$-learning，通过查询存储着各个状态-动作对的$Q$值来获得最优策略。由于要存储所有状态-动作对的$Q$值，所以这类方法适合状态和动作空间是离散的且维度较低的问题。对于状态或者动作空间是连续的问题，则可以结合深度神经网络来去近似状态值函数或者状态-动作值函数，这类方法属于深度强化学习方法。常见的比如A2C、TRPO、DDPG、SAC等，这类方法在动作空间维度不是很高的情况下，能很好地帮助代理找到最佳策略。

%但是随着动作空间维度变高，模型复杂度以及代理探索的时间复杂度都会变高，极大影响了


\subsection{深度多代理强化学习}
深度强化学习方法如A2C、TRPO、DDPG、SAC等大多用于处理单代理设置的问题，即环境中仅存在一个代理来与环境交互学习全局的决策。但是，现实生活中的很多场景都包含着多个代理， 如自动驾驶、包裹派送、车辆调度等问题。多代理设置下，代理们处在同一个环境中，每个代理根据其观测状态，独立地做出自己的决策，最大化全局目标或者个体目标。

多代理强化学习通过将问题建模为SC（Stochastic game，简称SG）\cite{SG}来形式化， SG可以用$<\mathcal{S}, \mathcal{A},\mathcal{P}, \mathcal{R},Z,\mathcal{O},n,\gamma>$定义，其中，$n$表示环境中代理的数量，用$u$来标识环境中的一个代理，$u\in\mathcal{U}\equiv\{1,..,n\}$。$\mathcal{S}$表示环境的全局状态，$A$是动作空间。在每个时间步， 每个代理$u$从联合动作$a\in A \equiv A^n$选择一个动作$a^u\in A$。环境根据其状态转移函数$\mathcal{P}(s^\prime |s,a^u) : \mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$进行相应的状态转移，每个代理所收到的奖励由奖励函数给出，$R = R(s,a, u):\mathcal{S}\times\mathcal{A}\times\mathcal{U}\to\mathbb{R}$。与单代设置一样，$\gamma\in[0,1]$是折扣因子。在局部可观测设置下，每个代理$u$会对应一个局部观测状态$o^u\in O$，该局部状态可以根据观测函数$Z(s,u):\mathcal{S}\times\mathcal{U}\to O$得到。本文默认是采用局部可观测设置，即每个代理仅可以感知环境的局部状态。

根据代理是否能完全感知全局状态，多代理强化学习的环境设置可以分为完全可观测设置和部分可观测设置，相应地，学习的策略也分为中心化策略和去中心化策略。中心化策略下一般对应于在完全可观测设置，其策略$\pi(a|s_t)$通过将全局状态$s_t$映射为联合动作$a$的概率分布，即：
\begin{equation}
	\pi(a|s_t):\mathcal{A}\times \mathcal{S}\to[0,1]
\end{equation}

但是，基于中心化策略的方法会面临两个挑战，首先，联合动作空间的维度会随着代理的数目呈指数形增长。其次，在真实世界的问题场景下，代理仅能观测到局部状态，故而学习中心化策略是不实际的。基于去中心化策略的方法则没有这个问题， 去中心化策略下，每个代理自身对应一个策略$\pi^u(a^u|s_t)$，将状态映射到该代理所对应动作的概率分布上。联合动作上的概率分布可由各代理的概率分布结合得到，形式化为：
\begin{equation}
P(a|s_t)=\prod_u\pi^u(a^u|s_t), u=1...n
\end{equation}

当基于多代理强化学习来最大化问题的全局目标时，代理之间需要达成协作，来最大化全局奖励。环境可以反馈给所有代理相同的奖励值来让代理达成完全合作的关系，即每个代理的奖励函数为：
\begin{equation}
r(s,a,u) = r(s,a,u^\prime), \forall u,u^\prime
\end{equation}

与单代理一样，每个代理需要最大化自身的期望回报：
\begin{equation}
J^u(\pi) = \mathbb{E_\pi}[\sum_{t=1}^T\gamma^{t-1}r_t^u]
\end{equation}

目前有很多方法可以处理多代理的强化学习问题，本文主要集中于深度多代理强化学习（Deep Learning for Multi Agent Reinforcement Learning，简称DMARL），DMARL结合了深度神经网络来表示各个代理的值函数和策略。在合作设置下，常见的让代理达成协作关系的方法是参数共享，即不同的代理使用相同的参数$\theta$来参数化其值函数和策略。参数共享极大节省了计算成本，所有代理的策略可以并行计算。同时，不同代理的经验都可以用来更新网络参数，提高了样本利用率。

Tan等人\cite{tampuu2017multiagent}通过结合IQL（Independent Q- leanrning，简称IQL）\cite{tan1993multi}和DQN\cite{mnih2015human}提出了比较经典的IDQN算法（Independent DQN，简称IDQN）。IDQN通过共享网络参数和使用索引来区别代理来扩展IQL算法，使其能应用到大规模多代理场景中。网络参数通过最小化以下的损失函数来更新：
\begin{equation}
\mathbb{E}[Q(s_t^u, a_t^u;\theta)-(r_{t+1}^u+\gamma\max_{a_{t+1}^u}Q(s_{t+1}^u, a_{t+1}^u;\theta^\prime))]
\end{equation}

其中$\theta^\prime$是目标Q网络的参数，$Q(s_t^u,a_t^u;\theta)$则表示代理$u$的状态-动作对$(s_t^u,a_t^u)$所对应的状态动作值，该方法一般用于动作空间是离散的问题，但很容易能扩展应用于连续的动作空间上。

\section{本章小结}
本章将动态定价问题进行了形式化建模并介绍其优化的目标函数，之后介绍了两个常用的动态定价方法，作为本文实验对比的基准算法之一。由于本文所提出的算法涉及强化学习的相关理论技术，所以本章同时介绍基于单代理的马尔科夫决策过程和深度多代理强化学习，并介绍局部观测和合作设置下，能让多代理达成合作的方法。为后续研究基于单代理和动态定价算法以及扩展到基于多代理的动态定价算法打下基础。

















