
\chapter{绪论}

\section{研究背景与意义}
网约车平台如滴滴出行、优步等平台自上线及不断发展以来，不仅满足了人们的日常出行需求，而且给乘客提供了方便快捷的出行服务和舒适优质的乘车环境。
根据中国最大的移动出行平台滴滴出行所发布的《滴滴出行企业公民报告2017》\cite{Report2017:didichuxing} 显示，2017年滴滴平台的网约车服务在400多个城市覆盖，日订单量达到2500万，在滴滴上获得收入的司机超2100万，为社会创建了巨大的经济效益和社会效益。不仅如此，由于出行市场品质化和专业化发展，网约车基本实现了连续增长。由CNNIC发布的第46次《中国互联网发展状况报告》 \cite{Report2020:CAC}显示，截止2020年6月，中国网民规模达到9.40亿，而网约车用户规模达3.40亿，占网民整体的36.2\%，这充分说明网约车已成为人们日常出行当中的不二选择。\par
网约车平台作为出行乘客和司机的中间媒介，负责将制定合适的订单价格，吸引有出行需求的乘客打车，激励司机加入平台去服务乘客，将尽可能多的订单与司机进行匹配，从已完成的订单提取部分收入作为其收入。因此，平台需要设计合适的乘车价格来服务更多乘客，提高平台的订单响应率和总收入。\par
在提高订单响应率和平台总收入方面面临的挑战之一就是如何更好地平衡供需。一方面，乘客和司机间的时空不匹配问题在实际生活中是普遍存在的。 例如，平峰期会出现供过于求的现象而在高峰期则是供不应求的情况。特别是在供不应求情况下，可用车辆被系统调度去服务距离其当前位置较远的乘客。结果，导致出现WGC现象（"Wild Goose Chase"）,司机会花费较长的时间去接乘客，一直处于忙碌状态，这将加剧空闲司机的稀缺性和交通的低效率\cite{castillo2017surge}。 另一方面，平台当前的决策会显式地影响未来司机的地理分布。如果平台使用短视的策略来制定当前的订单价格和车辆调度策略来平衡当前供需，忽略未来的供需状况，将导致未来可用司机的分布则会越来越分散，从而会造成未来需要用车的地区面临供应紧张的问题。 因此，平台需要设计具有前瞻性的策略，引导更多的司机未来出现在高需求的地区，提高未来可服务的订单数。\par
目前有很多的方法能够平衡供需，动态定价便是其中一项关键技术。不管是乘客还是司机对订单价格都比较敏感，比如，乘客更希望价格越低越好，如果价格超过其心理价位，那么乘客就会放弃打车；相反，司机则希望价格越高越好，这样就能提高其收入，价格越高，司机愿意加入平台并服务乘客的积极性就越高。 动态定价正是利用乘客和司机对价格的敏感性，基于实时的供需状况调整订单的价格，影响乘客和司机的行为。 动态定价对于缓解高峰负载均衡问题、提高订单调度率和最大化收入上有重要的作用。\par
本文研究如何根据实时的供需状况设计具有前瞻性的动态定价策略来最大化平台长期的累积订单响应率和累积收入。考虑到乘车的不稳定性和价格的后影响性，所设计的动态定价策略不仅要考虑平衡当前的供需，同时要尽可能平衡未来的供需。比如，当预知到未来某一地区会出现需求激增的情况时，当前的动态定价策略应该引导更多乘客前往该地区，从而使得未来能有更多的可用司机出现在该地区，避免未来该地区由于无车可用而造成订单流失或者需要远距离调车而导致WGC问题。前瞻性定价对于提高车辆的利用率和司机的收入，缩短乘客的等待时间，改善乘客的出行体验以及提高平台总收入具有重要意义。\par
大多数现有的网约车动态定价算法不具有前瞻性。优步和Lyft网约车平台采用的峰时定价作为其定价策略，当乘车需求高于车辆供应时，订单的基础价格就会乘上一个大于1的溢价系数，chen等人验证了峰时定价能激励司机在峰时工作更长时间\cite{chen2016dynamic}，Castillo等人则证明峰时定价能一定程度上缓解WGC问题\cite{castillo2017surge}。但是许多乘客在峰时会被收取昂贵的乘车费时，大部分司机也只有在峰时才愿意出现。Qian等人针对该问题提出了TOD定价策略，与峰时定价只有溢价系数不同，TOD还引入了折扣系数，虽然乘客在高峰时期的乘车价格会溢价，但是处于平峰时期时，平台会给出相应的折扣来补偿乘客\cite{article}。Bimpikis等人提出空间定价策略，即把城市划分成多个地区，订单的价格不仅与出发地区的供需有关，而且与目的地的供需有关。这些方法都致力于最大化平台当前的收入，并未兼顾平台未来的收入。Battifarano等人提出了一个峰时定价预测方法，通过构建机器学习模型建模交通流特征与峰时系数之间的关系，从而预测未来几分钟或几个小时内的峰时系数的变化，并且向司机和乘客共享预测结果，从而帮助平台有效地分配车辆，节省用户的金钱和时间\cite{battifarano2019predicting}，不过该方法并没有设计新的动态定价策略。Asghari等人则考虑了当前和未来两个时段的短期收入最大化，通过估计未来的需求和司机分布，在当前决策时则考虑引导更多司机出现在未来出现供需激增的地区\cite{asghari2018adapt}，但该方法只是兼顾了短期收入，如果想扩展模型兼顾到更长期（几个小时，一天）的收入，则面临计算复杂度高的问题。\par
强化学习通过研究环境中的代理如何选择动作以实现给定的目标\cite{sutton1998introduction}, 代理当前选择的动作考虑了即时的收益也考虑到了对未来的影响，以优化问题的长期目标。目前已经有不少使用强化学习来解决动态定价问题的相关研究\cite{rana2014real, lu2018dynamic,peters2013reinforcement,shen2020reinforcement,wang2019reinforcement},代理通过利用与环境互动收集来的经验学习提高定价策略。但是，这些方法把动态定价问题的价格（动作）离散化，之后使用传统的Q-learning方法求解，导致选择一个合适的离散动作数比较困难。如果离散动作数过于少，一个较大区间范围内的很多价格对代理来说都是一样的，代理收到关于价格动作的反馈也是不准确的，导致得到次优的策略解。同样地，如果离散动作数过大，模型的优化过程中也会面临维度灾难和计算负担过大的问题。并且，已有的基于强化学习的动态定价算法不是为网约车场景所设计的，其目标函数与网约车场景下的目标函数也不同，因此不能直接扩展这些方法用在网约车场景下。城市的交通环境是复杂多变的，而强化学习不需要事先了解城市的交通情况，仅通过与城市环境的不断交互就可以学习如何制定合适的价格策略，同时可以根据环境的变化适当的做出响应。所以，强化学习可以帮助智能化定价，而智能化定价能大大促进智慧交通的发展。




\section{国内外研究现状}
动态定价对于解决智能交通系统中的拥塞控制、流量负载均衡和网约车的车辆调度问题起着重要作用，被广泛研究并应用于各种交通问题，比如车费定价、停车费定价、拥塞定价等，来最大化乘车服务或者停车服务的提供者的收入、缓解高峰负载。网约车动态定价则属于车费定价，网约车平台使用不同的动态定价技术提高平台收入和平衡供需。随着各类网约车平台的发展和移动出行的普及，近年来出现了不少关于网约车动态定价的研究，如峰时定价、需求定价、分时定价等，这些方法或使用传统的基于数学优化的解来求解最佳价格，或基于深度学习和强化学习来优化订单价格。本文中涉及网约车动态定价和强化学习的内容，因此本节将简要介绍网约车动态定价技术和强化学习技术的国内外研究现状。
\subsection{动态定价}
现存的网约车动态定价研究有很多，一般的优化目标是最小化平台运营成本\cite{long2018ride}、最大化社会福利\cite{fang2019prices}或者最大化平台收入\cite{hu2016joint}等。Banerjee等人结合经济模型和排队模型来研究能使得平台收入最大化的最优价格，并对比了动态定价和静态定价两种定价策略下的吞吐量，发现动态定价能带来更多的吞吐量\cite{banerjee2015pricing}。zha等人提出了打车市场不同劳动力供给行为假设下的均衡模型，并考察了峰时定价的表现，结果发现与静态定价相比，动态定价能为平台和司机带来更高的收入\cite{zha2018surge}。峰时定价是动态定价较为流行的一种定价模型，已被应用到优步\cite{rempel2014review}和Lyft\cite{yan2019dynamic}等网约车平台上，峰时定价能激励司机在道路上工作更长时间\cite{chen2016dynamic}，解决WGC问题\cite{castillo2017surge}。以上的定价策略都旨在最大化平台收入，Fang等人发现基于收入最大化的定价模型会由于司机供应短缺而限制其收入，从而提出引入补贴来鼓励司机提供乘车服务。胡天宇\cite{hutianyu2020}等人基于排队论模拟司机在系统中的流动，并根据动态定价构建社会福利最大化模型以及平台收入最大化模型，对比两个模型发现平台利润最大化模型获得的平台定价更高。。
上述文献均考虑优化平台当前收入为研究目标，是一种短视的定价策略，为考虑当前的定价策略对平台未来的收入的影响。随着需求预测准确度的提高，未来几分钟内或1个小时内的乘车需求可以较为准确地估计\cite{yao2018deep,zhang2020spatial}。Guda等人通过共享平台对乘车市场的需求预测结果给司机，从而引导司机从供应过剩的区域转移到供应短缺的区域\cite{guda2019your}。Asghari等人结合下一时段的需求预测结果，通过降低当前时段某些地区的乘车价格，刺激更多前往未来供应短缺地区的乘车需求，从而能够使得更多司机出现在未来需求激增的地区，平衡未来的供需。但是这些模型仅考虑了当前和下一时段的短期收入优化问题，若需要将这些模型扩展到优化更长期的收入，则面临建模困难以及模型优化困难的问题。\par
强化学习能够优化长期目标，并且基于无模型的强化学习模型不需要建模环境的动态性，通过代理与环境的交互来地响应环境的变化，因此也被广泛应用于解决不同场景的动态定价问题。Kim等人\cite{kim2015dynamic}和Lu等人\cite{lu2018dynamic}利用强化学习来优化电力市场的价格平衡供需以降低能源消耗，强化学习无需关于电网系统环境的先验信息，解决动态定价中缺乏消费者信息和电网系统存在多种不确定性的问题。针对需求不平稳的收入管理问题，Rana等人\cite{rana2014real}提出一个无模型方法来自适应地响应需求的变化，最大化收入。Maestre等人\cite{maestre2018reinforcement}在利用强化学习指定合适的价格以权衡系统收入和差异性定价造成的不公平性。以上的这些方法使用Q-learning方法来优化定价策略，因此需要将价格按照一定的粒度进行离散化。但是，现实中大部分动态定价问题的都需要价格是连续的数值，将问题建模在离散动作空间会影响结果的准确度。Turan等人\cite{turan2019dynamic}则在连续的动作空间中建模电车调度和电价的定价问题，其目标是平台收入最大化，故而将即时收入作为奖励函数，之后使用TRPO优化一段时间内的累积收入。然而，使用即时收入作为奖励函数可能对于其它问题很适用，但是对于网约车市场，打车需求是不平衡的，收入不仅受到订单价格的影响，而且与乘车需求有关，因此使用即时收入作为奖励函数在网约车动态定价问题上不一定适合。
\subsection{强化学习}
近年来，强化学习已成为机器学习领域的一个研究热点，被广泛应用来解决金融\cite{jiang2017deep}、交通\cite{wei2018intellilight}、机器人控制\cite{nguyen2019review}等领域问题
强化学习是通过代理与动态环境交互来进行行为学习\cite{kaelbling1996reinforcement}，学习如何把状态映射到动作，以最大化数值奖励信号\cite{sutton1998introduction}。与监督学习不同，强化学习不需要训练数据，而是通过环境所反馈的奖励信号判断其所选择动作的好坏，从而做出相应的调整。强化学习其中一个挑战便是平衡探索和利用，一方面，代理需要充分地探索环境，获取更多信息避免局部优化；另一方面，代理也需要利用目前已经学习到的知识做出决策。常见的平衡探索和利用的方法有$\epsilon-$贪婪算法\cite{sutton1998introduction}、置信空间上限（Upper Confidence Bound，简称UCB）\cite{auer2002finite}、添加噪声项\cite{fortunato2017noisy}、添加熵正则项\cite{ahmed2019understanding}等。按照环境中代理的数目，强化学习模型可以分为单代理模型和多代理模型。
基于单代理的强化学习方法比较简单，适合动作空间维度不是特别大的问题，在平衡探索和利用上采用常用的方法即可。而当动作空间维度过高，基于单代理的方法即使结合深度学习技术，也需要花费大量的时间进行充分的探索，并且单个模型参数过多会导致收敛困难。而解决动作维度过高的一个解决方法是采用多代理的强化学习方法，将问题分解成一个个子问题，每个子问题由一个代理去处理，从而降低了问题的动作空间维度。基于多代理的强化学习需要各个代理间形成协作，共同去实现全局目标。常见的可以让代理们达成协同关系的方法有参数共享\cite{gupta2017cooperative}、中心化训练\cite{lowe2017multi}等。中心化训练会随着代理数目的增长而导致状态空间和动作空间的维度增长，从而导致训练和收敛困难。参数共享则所有代理共享单个策略的参数，状态空间和动作空间的维度不受代理数目的影响。%通过代理的局部观测和代理索引来得到各个代理不同行为，




\section{研究内容与创新点}
本文为网约车平台提出一种新的动态定价方法，以最大化长期的APR（Accumulative Platform Revenue，简称APR）和ORR（Order Response Rate，简称ORR）。订单响应率能够体现平台的服务水平，一般来说，订单响应率越高，收入也越高。为了找到前瞻性的定价策略，在决策时有必要考虑未来的供需关系。强化学习在做决策时权衡了当前收益与未来的收益，天然适合于解决最大化长期奖励的问题。之前大部分动态定价算法是基于数学优化，需要预先建模环境的动态性，强化学习不需要事先建模复杂的城市交通环境，仅通过代理与环境的交互来学习。强化学习与深度学习结合来近似动作值函数, 能避免动作空间离散化。因此，本文应用深度强化学习来求解网约车平台的动态定价问题。\par

基于深度强化学习的网约车动态定价算法需要解决以下的挑战，首先，需要设计适合的奖励函数，不合适的奖励函数会导致不收敛的策略。其次，不同的调度策略会得到不同的司机地理分布，有可能会使得目标函数陷入次优，因此控制可用司机的未来的地理分布是重要的。最后，由于本文将问题建模在连续的动作空间中，因此需要让代理充分地探索环境，避免局部最优。

为了进一步解决上述的第一个挑战，本文设计了一个新的奖励函数。基于其它场景的动态定价问题一般将即时收入作为奖励函数，在需求是平稳的场景下，直接将即时收入作为奖励函数是合适的。但是对于网约车场景下，需求是不平稳的，在时间和空间上均存在波动，这时，平台收入也受到需求的影响，即时收入作为奖励函数将导致错误的优化方向。所以需要另外设计合适的奖励函数。考虑到目标函数是最大化APR和ORR，本文的奖励函数包含了即时收入和订单响应率, 除了这两项以外，奖励函数还加上了下一时段各地区的司机供应分布和打车需求分布的KL散度，这一项能够让代理在做决策时兼顾到未来的供需状况。针对第二个问题，Chen等人\cite{chen2018pricing}提出只把订单价格作为唯一市场调节手段，价格会较高和不稳定。并且只优化价格并不能做到全局优化，且系统不能控制司机的未来分布。因此，本文将订单价格和司机调度联合优化。对于第三个问题，为了提高代理的探索能力，本文应用SAC（Soft Actor-Critic， 简称SAC）算法来优化定价策略。SAC算法通过最大化奖励函数的期望和策略熵来进行充分的探索，并且能保证模型的稳定性\cite{haarnoja2018soft}。
本文将基于深度强化学习的动态定价方法命名为DRLDP（Deep Reinforcement Learning for Dynamic Pricing）
。
DRLDP是基于单代理的方法，其代理的策略包含两部分，一部分是价格策略，另一部分是调度策略。本文将城市分成$N$个区域，订单价格根据其起点区域和终点区域的供需状况进行差异化定价，因此需要优化的订单价格有$N^2$个，同时，代理亦需要优化从一个区域调度去服务前往其它区域的订单的司机比例，所以调度比例也有$N^2$个需要优化。总的动作数为$2N^2$，该动作数会随着区域划分数的增长而增长，会给模型的优化和收敛带来困难。为了解决动作维度过高而导致的优化和收敛问题，本文另提出了基于多代理的强化学习方法MRLDP（Multi-agent Reinforcement Learning for Dynamic Pricing) ，每个区域前往其它区域的订单策略和调度策略有一个代理负责优化，这样就能把动作维度从$2N^2$降到$2N$，极大降低了动作维度，方便模型的收敛。MRLDP需要在多个代理间形成协同，共同优化全局的APR和ORR，为此，本文使用共享参数策略让所有代理形成协作。在共享参数下，每个代理输入自身的局部观测状态和索引，便能得到其对应的策略，同时所有代理与环境交互获得的经验被共同用来优化策略网络，因此能极大加速模型的收敛。

本文的研究内容和创新点总结如下：\par
（1） 针对之前的动态定价模型为优化长期收入来得到前瞻性的定价策略的问题，本文提出了一个深度强化学习框架DRLDP来解决网约车平台的动态定价问题。DRLDP将同时优化订单价格和车辆调度以谋求全局优化，并将动态定价问题建模在连续的动作空间上，并使用SAC算法找到最优策略。

（2）  对在乘车需求不稳定情况下直接将即时收入作为网约车动态定价并不适合的问题，本文设计了一个新的奖励函数。奖励函数包括即时收入、订单响应率和下一时间步的车辆供应分布和乘车需求分布之间的KL散度之和。

（3）  针对基于单代理的DRLDP会由于动作空间维度增长而导致优化和收敛困难的问题，本文另提出基于多代理的MRLDP方法来解决该问题，每个区域由一个代理负责其定价策略和调度策略的优化，使用共享参数让所有代理达成协同，来优化全局的APR和ORR。

（4）  本文在真实的数据集上与基准算法进行比较，验证了本文所提出的两个方法的性能。


\section{论文章节安排}

本论文的章节安排如下：

第一章：绪论，介绍了本文的研究背景与意义，所研究问题所涉及的动态定价领域和强化学习领域的国内外研究现状，以及本文的主要研究内容和创新点。

第二章：问题形式化和相关理论技术，将动态定价问题进行形式化，并介绍本文所涉及和应用到的相关理论和技术。

第三章：基于单代理的动态定价方法。详细介绍基于单代理的DRLDP方法，包括马尔科夫决策过程中的基本元素如状态、动作、奖励函数和状态转移函数的设计，并且展示在真实数据集上的实验结果。

第四章：基于多代理的动态定价方法。介绍为了解决动作维度增长而导致优化和收敛困难问题，提出的基于多代理的MRLDP方法，介绍各个代理的马尔科夫决策过程，以及促进代理间协作的方法。最后进行对比实验，验证该方法的性能。

第五章：总结与展望。对本文的研究工作做一个总结，发现其优点与不足，并对其不足和未来可改进之处做出展望。


